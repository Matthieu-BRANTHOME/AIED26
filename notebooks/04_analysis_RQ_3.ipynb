{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "![Logo AIED26](../assets/logo_AIED26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# **Notebook#04 : Analysis on RQ3**\n",
    "## [students' perception of the digital assistant]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1/ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Internal\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import students_constants as stu_const\n",
    "import interaction_constants as int_const\n",
    "import tests_constants  as tes_const\n",
    "import session_date_constants as ses_const\n",
    "\n",
    "# External\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## 2/ Data importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_data = pd.read_pickle(\"../data/interim/interaction_data.pkl\")\n",
    "pre_test_data = pd.read_pickle(\"../data/interim/pre_test_data.pkl\")\n",
    "post_test_data = pd.read_pickle(\"../data/interim/post_test_data.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 3/ Students' perception of the digital assistant [B/C]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "What is student perception of the assistant helps depending of groups ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns of questions to analyze (from _QA to _QE)\n",
    "question_cols = tes_const.ASSISTANT_NUM_QUESTIONS\n",
    "\n",
    "# Mapping of question keys to plain text question\n",
    "QUESTION_MAPPING = {\n",
    "    tes_const.QA_KEY: \"Did you find the assistant help useful to progress in the game?\",\n",
    "    tes_const.QB_KEY: \"Did you find the assistant help useful to learn Python programming?\",\n",
    "    tes_const.QD_KEY: \"Did you find the assistant help easy to understand?\",\n",
    "    tes_const.QE_KEY: \"Did you find that the assistant help was correct (without errors)?\",\n",
    "    tes_const.QC_KEY: \"Would you like to have more help from a digital assistant in the future?\"\n",
    "}\n",
    "\n",
    "group_col=tes_const.GROUP_ID_KEY\n",
    "# Filter only groups B and C\n",
    "df_filtered = post_test_data[post_test_data[group_col].isin([tes_const.GROUP_B, tes_const.GROUP_C])].copy()\n",
    "\n",
    "# Dictionary to store results\n",
    "stats_dict = {}\n",
    "\n",
    "# Loop over each question\n",
    "for q in question_cols:\n",
    "    # Get data for both groups\n",
    "    calls_B = df_filtered[df_filtered[group_col] == tes_const.GROUP_B][q].dropna()\n",
    "    calls_C = df_filtered[df_filtered[group_col] == tes_const.GROUP_C][q].dropna()\n",
    "    \n",
    "    # Descriptive statistics\n",
    "    mean_B, std_B = calls_B.mean(), calls_B.std()\n",
    "    mean_C, std_C = calls_C.mean(), calls_C.std()\n",
    "    n_B, n_C = len(calls_B), len(calls_C)\n",
    "    \n",
    "    print(f\"\\n######## {q} : {QUESTION_MAPPING[q]} ########\")\n",
    "    print(\"=== DESCRIPTIVE STATISTICS ===\")\n",
    "    print(f\"Group B (n={n_B}): M = {mean_B:.2f}, SD = {std_B:.2f}\")\n",
    "    print(f\"Group C (n={n_C}): M = {mean_C:.2f}, SD = {std_C:.2f}\")\n",
    "    \n",
    "    # Normality tests (Shapiro-Wilk)\n",
    "    shapiro_B = stats.shapiro(calls_B) if n_B >= 3 else None\n",
    "    shapiro_C = stats.shapiro(calls_C) if n_C >= 3 else None\n",
    "    \n",
    "    alpha = 0.05\n",
    "    print(\"\\n=== NORMALITY TESTS (Shapiro-Wilk) ===\")\n",
    "    if shapiro_B and shapiro_C:\n",
    "        print(f\"Group B: p-value = {shapiro_B.pvalue:.4f} -> {'normally distributed' if shapiro_B.pvalue >= alpha else 'not normally distributed'}\")\n",
    "        print(f\"Group C: p-value = {shapiro_C.pvalue:.4f} -> {'normally distributed' if shapiro_C.pvalue >= alpha else 'not normally distributed'}\")\n",
    "        normal_B, normal_C = shapiro_B.pvalue >= alpha, shapiro_C.pvalue >= alpha\n",
    "    else:\n",
    "        # Not enough data for Shapiro test\n",
    "        print(\"Not enough data for Shapiro-Wilk test\")\n",
    "        normal_B, normal_C = False, False\n",
    "    \n",
    "    # Levene test for equal variances\n",
    "    if n_B > 1 and n_C > 1:\n",
    "        levene_p = stats.levene(calls_B, calls_C).pvalue\n",
    "        equal_var = levene_p >= alpha\n",
    "        print(f\"\\n=== HOMOGENEITY OF VARIANCES (Levene) ===\")\n",
    "        print(f\"Levene p-value = {levene_p:.4f} -> {'variances are similar' if equal_var else 'variances differ'}\")\n",
    "    else:\n",
    "        equal_var = False\n",
    "        print(\"\\nNot enough data for Levene test\")\n",
    "    \n",
    "    # Choose statistical test\n",
    "    print(f\"\\n=== STATISTICAL TEST ===\")\n",
    "    if normal_B and normal_C and equal_var:\n",
    "        print(\"Using independent t-test (parametric)\")\n",
    "        stat, p_value = stats.ttest_ind(calls_B, calls_C, equal_var=True)\n",
    "        # Cohen's d\n",
    "        pooled_std = np.sqrt(((n_B - 1)*std_B**2 + (n_C - 1)*std_C**2) / (n_B + n_C - 2))\n",
    "        effect_size = (mean_B - mean_C) / pooled_std\n",
    "        effect_type = \"Cohen's d\"\n",
    "\n",
    "    else:\n",
    "        print(\"Using Mann-Whitney U test (non-parametric)\")\n",
    "        stat, p_value = stats.mannwhitneyu(calls_B, calls_C, alternative='two-sided')\n",
    "        # Effect size r = Z / sqrt(N)\n",
    "        n_total = n_B + n_C\n",
    "        z_score = stats.norm.ppf(1 - p_value/2)  # convert two-sided p-value to Z\n",
    "        effect_size = abs(z_score) / np.sqrt(n_total)\n",
    "        effect_type = \"r\"\n",
    "\n",
    "    print(f\"Test statistic = {stat:.4f}\")\n",
    "    print(f\"P-value = {p_value:.4f}\")\n",
    "    print(f\"Effect size ({effect_type}) = {effect_size:.4f}\")\n",
    "    \n",
    "    # Interpret effect size\n",
    "    if effect_type == \"Cohen's d\":\n",
    "        if abs(effect_size) < 0.2:\n",
    "            label = \"negligible\"\n",
    "        elif abs(effect_size) < 0.5:\n",
    "            label = \"small\"\n",
    "        elif abs(effect_size) < 0.8:\n",
    "            label = \"medium\"\n",
    "        else:\n",
    "            label = \"large\"\n",
    "    else:  # r\n",
    "        if effect_size < 0.1:\n",
    "            label = \"negligible\"\n",
    "        elif effect_size < 0.3:\n",
    "            label = \"small\"\n",
    "        elif effect_size < 0.5:\n",
    "            label = \"medium\"\n",
    "        else:\n",
    "            label = \"large\"\n",
    "    \n",
    "    print(f\"Effect size interpretation: {label}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(f\"SIGNIFICANT difference: p = {p_value:.4f} < {alpha}\")\n",
    "    else:\n",
    "        print(f\"No significant difference: p = {p_value:.4f} >= {alpha}\")\n",
    "    \n",
    "    # Store results in dictionary\n",
    "    stats_dict[q] = {\n",
    "        \"mean_B\": mean_B,\n",
    "        \"std_B\": std_B,\n",
    "        \"mean_C\": mean_C,\n",
    "        \"std_C\": std_C,\n",
    "        \"significant\": p_value < alpha\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping of question keys to English question text\n",
    "QUESTION_DISPLAY = {\n",
    "    tes_const.QA_KEY: \"Useful to progress in the game?\",\n",
    "    tes_const.QB_KEY: \"Useful to learn Python programming?\",\n",
    "    tes_const.QD_KEY: \"Help easy to understand?\",\n",
    "    tes_const.QE_KEY: \"Help was correct (without errors)?\",\n",
    "    tes_const.QC_KEY: \"Digital assistant in the future?\"\n",
    "}\n",
    "\n",
    "# Use previously computed stats_dict\n",
    "summary_list = []\n",
    "for q in question_cols:\n",
    "    stats_q = stats_dict[q]  # fetch stored results\n",
    "    \n",
    "    summary_list.append({\n",
    "        'question': q,\n",
    "        'question_text': QUESTION_DISPLAY[q],\n",
    "        'mean_B': stats_q['mean_B'],\n",
    "        'std_B': stats_q['std_B'],\n",
    "        'mean_C': stats_q['mean_C'],\n",
    "        'std_C': stats_q['std_C'],\n",
    "        'significant': stats_q['significant']\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_list)\n",
    "\n",
    "# Parameters for side-by-side bar chart\n",
    "x = np.arange(len(summary_df))\n",
    "width = 0.40\n",
    "\n",
    "# Create figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot bars with error bars\n",
    "bars_B = ax.bar(x - width/2, summary_df['mean_B'], width, yerr=summary_df['std_B'], capsize=5, label='Group B (Free-Content)', color='skyblue')\n",
    "bars_C = ax.bar(x + width/2, summary_df['mean_C'], width, yerr=summary_df['std_C'], capsize=5, label='Group C (Constrained-Content)', color='salmon')\n",
    "\n",
    "# Set x-axis labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary_df['question_text'], rotation=25, ha='right', fontsize=10)\n",
    "\n",
    "# Add axis labels and title\n",
    "# ax.set_xlabel(\"Question\", fontsize=12)\n",
    "ax.set_ylabel(\"Mean score\", fontsize=12)\n",
    "# Add horizontal dashed line at 50%\n",
    "ax.axhline(y=50, color='red', linestyle='--', linewidth=1.5, label='50% reference')\n",
    "\n",
    "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "ax.set_ylim(0, 100)\n",
    "\n",
    "# Annotate significance and values above bars\n",
    "for i, row in summary_df.iterrows():\n",
    "    # Position for significance star\n",
    "    y_star = max(row['mean_B'], row['mean_C']) + max(row['std_B'], row['std_C']) * 0.1\n",
    "    if row['significant']:\n",
    "        ax.text(i, y_star, '*', ha='center', va='bottom', fontsize=16, color='black')\n",
    "\n",
    "    # Add values above each bar (just above the bar, ignoring SD)\n",
    "    offset = 1.4  # small vertical margin above the bar\n",
    "    dx_b = 0.17   # horizontal shift (can adjust if needed)\n",
    "    dx_c = -0.17  # horizontal shift (can adjust if needed)\n",
    "    ax.text(i - width/2 - dx_b, row['mean_B'] + offset, f\"{row['mean_B']:.1f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
    "    ax.text(i + width/2 - dx_c, row['mean_C'] + offset, f\"{row['mean_C']:.1f}\", ha='center', va='bottom', fontsize=10, color='black')\n",
    "\n",
    "# Add legend\n",
    "ax.legend()\n",
    "\n",
    "# Adjust layout, save figure, and display\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../outputs/assistance_perception_BC.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
